{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/STASYA00/IAAC2024_tutorials/blob/main/quickstarts/05_image2image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> - Stasja's notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-to-Image translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚙️ Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
    "%cd pytorch-CycleGAN-and-pix2pix\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⛄️Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ☃️ Existing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few existing paired image datasets. Some of the most famous ones:\n",
    "\n",
    "* [CMP Facade Database](https://cmp.felk.cvut.cz/~tylecr1/facade/)\n",
    "* 🤩[Cityscapes](https://www.cityscapes-dataset.com/)\n",
    "* [Satellite Imagery](https://www.kaggle.com/datasets/alincijov/pix2pix-maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download and structure a sample dataset from [the repo](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ./datasets/download_pix2pix_dataset.sh facades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ☃️ Custom datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom dataset for training pix2pix should contain paired images, where pixels in set A would correspond to _converted_ pixels in B.\n",
    "\n",
    "At first your images should be in two folders:\n",
    "- Set A\n",
    "- Set B\n",
    "\n",
    "These images need to be stitched together and split into `train`, `validation` and `test` sets, making sure that the distribution in these sets in more or less similar. \n",
    "\n",
    "Final folder structure:\n",
    "\n",
    "    .\n",
    "    ├── custom_dataset\n",
    "    │   ├── train\n",
    "    │   │   ├── img1.jpg\n",
    "    │   │   └── img2.jpg\n",
    "    │   ├── validation\n",
    "    │   │   ├── img3.jpg\n",
    "    │   │   └── img4.jpg\n",
    "    │   └── test\n",
    "    │       ├── img5.jpg\n",
    "    │       └── img6.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Pix2Pix dataset image](../.assets/pix2pix_sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🫧 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the following arguments:\n",
    "\n",
    "* `--dataroot`  - Path to your dataset __root__ folder (where train, test, val are), e.g. `./datasets/facades`\n",
    "* `--name`      - Name of the folder where the checkpoints and intermediate results would be stored (if the name is `xxx`, the models will be stored at `checkpoints/xxx`, the images at `checkpoints/web/images/`)\n",
    "* `--model`     - Model to train with, [cycle_gan | pix2pix | test | colorization]\n",
    "* `--direction` - in which direction the images are translated, whether it is A->B or B->A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all the possible arguments by running `!train.py --help`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA --display_id -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Original paper](https://arxiv.org/abs/1611.07004)\n",
    "* [Original Colab](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb#scrollTo=vrdOettJxaCc)\n",
    "* [Tensorflow training](https://www.tensorflow.org/tutorials/generative/pix2pix)\n",
    "* [Demo](https://affinelayer.com/pixsrv/)\n",
    "* [Coursera GANs Specialization course](https://www.coursera.org/specializations/generative-adversarial-networks-gans)\n",
    "* [Tensorflow pix2pix implementation](https://github.com/affinelayer/pix2pix-tensorflow)\n",
    "* [PyTorch pix2pix implementation](https://github.com/mrzhu-cool/pix2pix-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
